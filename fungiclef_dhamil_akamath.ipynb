{"cells":[{"cell_type":"markdown","id":"852b25bc","metadata":{"id":"852b25bc"},"source":["# **CS482 Final Project**\n","### **Written by:** Diane Hamilton and Aadya Kamath\n","### **Due:** May 13, 2025"]},{"cell_type":"markdown","source":["## **Topic:** FungiCLEF25 @ CVPR-FGVC & LifeCLEF\n","\n","##**Overview**\n","The FungiCLEF Challenge focuses on few-shot recognition of fungi species using real-world observational data. Each observation includes multiple photographs of the same specimen, along with metadata (e.g., location, timestamp, substrate, habitat, toxicity), satellite imagery and meteorological variables.\n","<p>\n","The goal of the challenge is to develop a classification model capable of returning a ranked list of predicted species for each observation. A key challenge lies in handling a large number of species consisted of rare and under-recorded taxa with very few training examples.\n","\n","**Input:** A list of fungi observations.\n","**Output:** A list of Top-k predicted fungi species from a predefined set of classes.\n","\n","**Kagle Link**: https://www.kaggle.com/competitions/fungi-clef-2025\n"],"metadata":{"id":"O9MRajZBfOZx"},"id":"O9MRajZBfOZx"},{"cell_type":"markdown","source":["# ðŸ”¬ **0: Setup Steps**\n","\n","In this section, we carry out installations that are necessary for performing the tasks we will be performing in this project.\n","\n","##**ðŸ“Œ Tools to keep in mind**\n","\n","PyTorch or TensorFlow for modeling\n","\n","Hugging Face transformers for text\n","\n","sklearn/pandas for tabular preprocessing\n","\n","albumentations or timm for data augmentation\n","\n","Faiss for embedding similarity search (rare species)"],"metadata":{"id":"3gZX6ZRomUjK"},"id":"3gZX6ZRomUjK"},{"cell_type":"code","source":["#pip install our desired model\n","#pip install other packages or libraries we need\n","\n","import os\n","from PIL import Image"],"metadata":{"id":"wboG4dA3mTcQ","executionInfo":{"status":"ok","timestamp":1746319387709,"user_tz":240,"elapsed":24,"user":{"displayName":"diane hamilton","userId":"08854483884915150742"}}},"id":"wboG4dA3mTcQ","execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["##ðŸ¤”**1: Understand the Problem**\n","\n","**Goal:** Rank fungi species per observation (i.e., not just top-1 accuracy).\n","\n","**Challenge:** Many classes (2,000+), most with very few examples (1â€“4).\n","\n","**Opportunity:** Rich metadata (temporal, spatial, environmental) and image captions to augment vision data."],"metadata":{"id":"NrKhPWv7ix71"},"id":"NrKhPWv7ix71"},{"cell_type":"code","source":["# visualize img and tabular data\n","PATH = '/images/FungiTastic-FewShot'\n","TRAIN_PATH = '/images/FungiTastic-FewShot/train'\n","TEST_PATH = '/images/FungiTastic-FewShot/test'\n","RESOLUTION = '300' + 'p'\n","\n","files = os.listdir(TRAIN_PATH)\n","for i in range(5):\n","  fpath = os.path.join(TRAIN_PATH, files[i])\n","  print(fpath)"],"metadata":{"id":"dxUnZOPbkLrz","colab":{"base_uri":"https://localhost:8080/","height":210},"executionInfo":{"status":"error","timestamp":1746319550258,"user_tz":240,"elapsed":138,"user":{"displayName":"diane hamilton","userId":"08854483884915150742"}},"outputId":"6b372da7-75ef-430a-b7f1-6384ca12e54c"},"id":"dxUnZOPbkLrz","execution_count":5,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[WinError 3] The system cannot find the path specified: '/images/FungiTastic-FewShot/train'","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m TEST_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/images/FungiTastic-FewShot/test\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m RESOLUTION \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m300\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      9\u001b[0m   fpath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TRAIN_PATH, files[i])\n","\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/images/FungiTastic-FewShot/train'"]}]},{"cell_type":"code","source":["for i in range(5):\n","  with Image.open(fpath) as img:\n","              img.show()"],"metadata":{"id":"P9fTNVrxuF-N"},"id":"P9fTNVrxuF-N","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##â›½**2: Preprocess & Explore the Data**\n","\n","**Image sets:** Start with 300px or 500px images for quicker experimentation.\n","\n","**Metadata:** Normalize categorical fields (habitat, substrate), encode timestamps (month/day), and parse EXIF features.\n","\n","**Satellite images & captions:** Treat these as separate input modalities for later fusion.\n","\n","**Class imbalance:** Compute class frequencies and identify rare classes."],"metadata":{"id":"5bmMYy83kLYD"},"id":"5bmMYy83kLYD"},{"cell_type":"code","source":["# perform preprocessing on img data"],"metadata":{"id":"wmjC_YVYeSMD"},"id":"wmjC_YVYeSMD","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# perform preprocessing on tabular data"],"metadata":{"id":"q1l2RNsglu_p"},"id":"q1l2RNsglu_p","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# (optional) encode metadata"],"metadata":{"id":"p89__jBtlwR2"},"id":"p89__jBtlwR2","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**3: Base Modeling**\n","\n","**Image model:** Use a pre-trained image encoder (e.g. EfficientNet, ConvNeXt, or a ViT variant) fine-tuned on your dataset.\n","\n","**Metadata model:** MLP over tabular features (or transformer if complex). Normalize numerical fields.\n","\n","**Combine:** Late fusion â€” combine image and metadata logits before ranking predictions.\n","\n","**Loss:** Use cross-entropy with class re-weighting or focal loss to boost rare class signal."],"metadata":{"id":"StDVsWKbl4Mi"},"id":"StDVsWKbl4Mi"},{"cell_type":"code","source":["# base model"],"metadata":{"id":"k_MUUk6SnRvc"},"id":"k_MUUk6SnRvc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**4: Improve Recall@5 (Core Metric)**\n","\n","**Multi-label softmax + top-k selection:** Ensure your model predicts scores for all classes, then evaluate Top-5 accuracy.\n","\n","**Labelling:** Apply label smoothing or mixup to improve generalization.\n","\n","**Classification:** Use cosine classifier or ArcFace for better separation in embedding space."],"metadata":{"id":"B4CdPXUqnUS4"},"id":"B4CdPXUqnUS4"},{"cell_type":"code","source":["# improve recall"],"metadata":{"id":"VFf3oniIneC9"},"id":"VFf3oniIneC9","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**5: Handling Rare Classes**\n","\n","**Data augmentation:** Use strong augmentations (RandAugment, CutMix, etc.).\n","\n","**External data:** Use semi-supervised learning (e.g., pseudo-labeling) if allowed by rules.\n","\n","***Use class prototypes or nearest neighbor matching from image embeddings to help model similarity across species.*"],"metadata":{"id":"sJKatiYLn_4J"},"id":"sJKatiYLn_4J"},{"cell_type":"code","source":["# rare cases"],"metadata":{"id":"WFH2A6yRoD8K"},"id":"WFH2A6yRoD8K","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**6: Captions & Satellite Data**\n","\n","**Captions:** Encode using a text transformer (e.g., BERT or CLIP text encoder) and fuse with image features.\n","\n","**Satellite data:** Use a small CNN or ViT variant to extract environmental embeddings.\n","\n","**Fusion:** Concatenate embeddings (early fusion) or fuse logits (late fusion). Try attention-based fusion for best performance."],"metadata":{"id":"k49U3xx7oU6Q"},"id":"k49U3xx7oU6Q"},{"cell_type":"code","source":[],"metadata":{"id":"bxbJs6gAoZ4f"},"id":"bxbJs6gAoZ4f","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**7: Train & Validate**\n","\n","**Stratified sampling for cross-validation:** Ensure rare classes are seen.\n","\n","**Train:** Train with multi-task loss (e.g., predict both species and toxicity) for auxiliary signal.\n","\n","**Recall@5:** Evaluate with Recall@5 consistently â€” this is your main optimization target.\n","\n"],"metadata":{"id":"v1p7wvgzodZo"},"id":"v1p7wvgzodZo"},{"cell_type":"code","source":[],"metadata":{"id":"hBgaaCtnon9e"},"id":"hBgaaCtnon9e","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**8: Submission Pipeline**\n","\n","For each observation:\n","\n","Aggregate predictions across multiple images.\n","\n","Return top-5 ranked species labels.\n","\n","Format correctly using the sample submission file."],"metadata":{"id":"NQzO3g9Vor2-"},"id":"NQzO3g9Vor2-"},{"cell_type":"code","source":[],"metadata":{"id":"O54azr4co1m0"},"id":"O54azr4co1m0","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}